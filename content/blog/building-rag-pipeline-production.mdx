---
title: "Building a Production RAG Pipeline: From POC to 50K Daily Queries"
date: "2024-03-28"
excerpt: "How we scaled our RAG system from a proof-of-concept to handling 50,000+ daily queries with 99.9% uptime. Learn about vector database selection, embedding strategies, and the infrastructure challenges we solved."
author: "Subrahmanya K P"
tags: ["AI", "RAG", "Vector Database", "MLOps", "LLM", "Infrastructure"]
coverImage: "/blog/rag-pipeline.png"
featured: true
series: "RAG at Scale"
seriesOrder: 1
---

# Building a Production RAG Pipeline: From POC to 50K Daily Queries

When we started building our Retrieval-Augmented Generation (RAG) system, we thought the hard part was getting the LLM to return accurate results. We were wrong. The real challenge was scaling from 10 test queries to 50,000+ production queries daily while maintaining sub-second latency.

## The Journey from POC to Production

### Phase 1: The Naive Implementation (Failed Spectacularly)

Our initial setup:
```python
# What NOT to do
embeddings = []
for doc in documents:
    embedding = openai.Embedding.create(input=doc)
    embeddings.append(embedding)
    
# Linear search through embeddings
results = []
for emb in embeddings:
    similarity = cosine_similarity(query_embedding, emb)
    if similarity > threshold:
        results.append(emb)
```

**Result**: 30-second response times with just 10,000 documents. Complete failure.

### Phase 2: Enter Vector Databases

After evaluating Pinecone, Weaviate, Qdrant, and Milvus, we chose Qdrant for its:
- Self-hosting capabilities
- Consistent performance at scale
- Built-in filtering without performance degradation

## Our Production Architecture

```yaml
# Simplified architecture
components:
  ingestion:
    - Document Processor (Python)
    - Embedding Service (GPU cluster)
    - Qdrant Vector DB (3-node cluster)
  
  serving:
    - API Gateway (Kong)
    - Query Service (FastAPI)
    - Cache Layer (Redis)
    - LLM Service (vLLM)
```

## Key Lessons Learned

### 1. Embedding Strategy Matters

```python
# Hybrid embedding approach
def create_embeddings(document):
    # Dense embeddings for semantic search
    dense_emb = sentence_transformer.encode(document)
    
    # Sparse embeddings for keyword matching
    sparse_emb = bm25_encoder.encode(document)
    
    return {
        'dense': dense_emb,
        'sparse': sparse_emb,
        'metadata': extract_metadata(document)
    }
```

### 2. Chunking is Critical

We tested multiple chunking strategies:
- Fixed-size chunks: Fast but broke context
- Sentence-based: Better context but inconsistent sizes
- Semantic chunks: Best results but computationally expensive

Our solution: Hybrid approach with overlapping windows
```python
def semantic_chunk(text, max_tokens=512, overlap=50):
    sentences = nltk.sent_tokenize(text)
    chunks = []
    current_chunk = []
    current_tokens = 0
    
    for sentence in sentences:
        tokens = count_tokens(sentence)
        if current_tokens + tokens > max_tokens:
            chunks.append(' '.join(current_chunk))
            # Keep overlap
            current_chunk = current_chunk[-2:]
            current_tokens = count_tokens(' '.join(current_chunk))
        
        current_chunk.append(sentence)
        current_tokens += tokens
    
    return chunks
```

### 3. Vector Database Optimization

Critical Qdrant configurations for production:
```yaml
collection_config:
  vectors:
    size: 1536  # OpenAI ada-002 dimensions
    distance: Cosine
  
  optimizers_config:
    deleted_threshold: 0.2
    vacuum_min_vector_number: 1000
    
  hnsw_config:
    m: 16
    ef_construct: 200
    full_scan_threshold: 10000
```

### 4. Caching Strategy

Three-tier caching saved us 70% on compute costs:
```python
@cache_manager
async def get_rag_response(query: str):
    # L1: Exact match cache (Redis)
    if exact_match := await redis.get(f"exact:{query}"):
        return exact_match
    
    # L2: Semantic similarity cache
    similar_queries = await find_similar_cached_queries(query)
    if similar_queries:
        return similar_queries[0].response
    
    # L3: Vector search with cached embeddings
    embedding = await get_or_compute_embedding(query)
    results = await vector_search(embedding)
    
    response = await generate_llm_response(query, results)
    await cache_response(query, embedding, response)
    
    return response
```

## Performance Metrics

After optimization:
- **Latency**: p50: 120ms, p95: 450ms, p99: 800ms
- **Throughput**: 50,000+ queries/day with 3 API nodes
- **Accuracy**: 94% relevant responses (human evaluated)
- **Cost**: $0.003 per query (including LLM costs)

## Infrastructure Deep Dive

### Vector Database Cluster Setup

```bash
# Qdrant cluster with data persistence
docker-compose.yml:
  qdrant-node-1:
    image: qdrant/qdrant:v1.7.4
    volumes:
      - ./data/node1:/qdrant/storage
    environment:
      - QDRANT__CLUSTER__ENABLED=true
      - QDRANT__CLUSTER__P2P__PORT=6335
    
  qdrant-node-2:
    # Similar config
    
  qdrant-node-3:
    # Similar config
    
  nginx:
    # Load balancer for the cluster
```

### Monitoring and Observability

Key metrics we track:
```python
# Custom metrics for RAG pipeline
metrics = {
    'embedding_latency': Histogram(),
    'vector_search_latency': Histogram(),
    'llm_generation_latency': Histogram(),
    'cache_hit_rate': Gauge(),
    'relevance_score': Histogram(),
}
```

## Common Pitfalls to Avoid

1. **Not planning for data updates**: Implement incremental indexing from day one
2. **Ignoring embedding drift**: Monitor and retrain embeddings regularly
3. **Over-relying on similarity scores**: Combine with metadata filtering
4. **Neglecting security**: Implement proper access controls for sensitive documents

## What's Next?

In the next post, I'll dive deep into our experiments with different embedding models and how we achieved a 40% improvement in retrieval accuracy by fine-tuning our own embeddings.

## Resources

- [Our open-source RAG evaluation framework](https://github.com/example/rag-eval)
- [Qdrant optimization guide](https://qdrant.tech/documentation/guides/optimization/)
- [Vector database benchmark results](https://ann-benchmarks.com/)

---

*Have questions about building RAG systems at scale? Feel free to reach out!*